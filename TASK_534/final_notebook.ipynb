{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def V(x, y):\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "def gradient(x, y):\n",
    "    gx = 4*x*(x**2 + y - 11) + 2*(x + y**2 - 7)\n",
    "    gy = 2*(x**2 + y - 11) + 4*y*(x + y**2 - 7)\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "def numerical_gradient(f, x, y, h=1e-6):\n",
    "    df_dx = (f(x + h, y) - f(x - h, y)) / (2*h)\n",
    "    df_dy = (f(x, y + h) - f(x, y - h)) / (2*h)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# âœ… Gradient check (use a non-stationary point)\n",
    "x_check, y_check = 1.0, 1.0\n",
    "g_analytical = gradient(x_check, y_check)\n",
    "g_numerical = numerical_gradient(V, x_check, y_check)\n",
    "num_norm = np.linalg.norm(g_numerical)\n",
    "gradient_check_error = (\n",
    "    np.linalg.norm(g_analytical - g_numerical) / num_norm if num_norm > 1e-12 else 0.0\n",
    ")\n",
    "\n",
    "# Gradient Descent\n",
    "eta = 1e-3\n",
    "max_iter = 50000\n",
    "tol = 1e-6\n",
    "x, y = 0.0, 0.0\n",
    "trajectory = [(x, y)]\n",
    "\n",
    "for i in range(max_iter):\n",
    "    g = gradient(x, y)\n",
    "    if np.linalg.norm(g) < tol:\n",
    "        break\n",
    "    x -= eta * g[0]\n",
    "    y -= eta * g[1]\n",
    "    trajectory.append((x, y))\n",
    "\n",
    "final_point = np.array([x, y])\n",
    "final_value = V(x, y)\n",
    "iterations = len(trajectory)\n",
    "\n",
    "# Hessian analytic\n",
    "def hessian(x, y):\n",
    "    hxx = 12*x**2 + 4*y - 42\n",
    "    hxy = 4*x + 4*y\n",
    "    hyy = 12*y**2 + 4*x - 26\n",
    "    return np.array([[hxx, hxy], [hxy, hyy]])\n",
    "\n",
    "H_analytic = hessian(x, y)\n",
    "\n",
    "# Hessian numerical via finite differences of gradient\n",
    "def numerical_hessian(grad_func, x, y, h=1e-5):\n",
    "    g_xph = grad_func(x + h, y)\n",
    "    g_xmh = grad_func(x - h, y)\n",
    "    g_yph = grad_func(x, y + h)\n",
    "    g_ymh = grad_func(x, y - h)\n",
    "    d2x = (g_xph[0] - g_xmh[0]) / (2*h)\n",
    "    d2xy = (g_yph[0] - g_ymh[0]) / (2*h)\n",
    "    d2yx = (g_xph[1] - g_xmh[1]) / (2*h)\n",
    "    d2y = (g_yph[1] - g_ymh[1]) / (2*h)\n",
    "    return np.array([[d2x, d2xy], [d2yx, d2y]])\n",
    "\n",
    "H_numerical = numerical_hessian(gradient, x, y)\n",
    "hessian_valid = np.allclose(H_analytic, H_numerical, atol=1e-5)\n",
    "\n",
    "# Eigenvalue and conditioning analysis\n",
    "eigvals, eigvecs = np.linalg.eig(H_analytic)\n",
    "lambda_min = float(np.min(eigvals))\n",
    "lambda_max = float(np.max(eigvals))\n",
    "condition_number = float(abs(lambda_max / lambda_min))\n",
    "is_positive_definite = bool(np.all(eigvals > 0))\n",
    "\n",
    "# Optimization report\n",
    "optimization_report = {\n",
    "    \"converged\": bool(np.linalg.norm(gradient(x, y)) < tol),\n",
    "    \"iterations\": iterations,\n",
    "    \"final_point\": final_point.tolist(),\n",
    "    \"final_value\": float(final_value),\n",
    "    \"gradient_check_error\": float(gradient_check_error),\n",
    "    \"hessian_valid\": bool(hessian_valid),\n",
    "    \"is_positive_definite\": is_positive_definite,\n",
    "    \"condition_number\": float(condition_number)\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
